{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13936466,"sourceType":"datasetVersion","datasetId":8881634}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/devamagarwal/sculpt-former?scriptVersionId=283218748\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# === SETUP & DATA ===\nimport os\nimport shutil\n\ndef setup_environment():\n    print(\"Installing dependencies...\")\n    os.system('pip install -q \"numpy==1.26.4\" \"scipy==1.11.4\" \"rasterio\" \"albumentations\" \"segmentation-models-pytorch\" \"tqdm\" \"torch_ema\"')\n    \n    if not os.path.exists('mados'):\n        print(\"Cloning repository...\")\n        os.system('git clone https://github.com/gkakogeorgiou/mados.git')\n\n    if not os.path.exists('./data/MADOS'):\n        print(\"Copying dataset...\")\n        os.makedirs('./data/MADOS', exist_ok=True)\n        src = '/kaggle/input/mados-dataset-new' \n        if os.path.exists(os.path.join(src, 'MADOS')): src = os.path.join(src, 'MADOS')\n        os.system(f'cp -r {src}/* ./data/MADOS/')\n\n    if not os.path.exists('./data/MADOS_nearest'):\n        print(\"Stacking bands...\")\n        os.system('python mados/utils/stack_patches.py --path ./data/MADOS')\n        print(\"Stacking complete.\")\n    else:\n        print(\"Data ready.\")\n\nif __name__ == \"__main__\":\n    setup_environment()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:24:41.056324Z","iopub.execute_input":"2025-12-01T05:24:41.056842Z","iopub.status.idle":"2025-12-01T05:30:52.364695Z","shell.execute_reply.started":"2025-12-01T05:24:41.056816Z","shell.execute_reply":"2025-12-01T05:30:52.364023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CELL 2 ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.losses import LovaszLoss\nimport rasterio\nimport numpy as np\nimport albumentations as A\nimport os\nimport random\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom torch_ema import ExponentialMovingAverage # <--- NEW: EMA\n\n# --- CONFIGURATION ---\nCONFIG = {\n    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"BATCH_SIZE\": 2,\n    \"EPOCHS\": 40,             # EXTENDED: 40 Epochs\n    \"NUM_CLASSES\": 16,\n    \"LR\": 6e-5,\n    \"VSCP_PROB\": 0.5,\n    \"PHASE_SWITCH_EPOCH\": 15  # Switch to Lovasz later (Epoch 15)\n}\n\nprint(f\"Configuration Loaded. Device: {CONFIG['DEVICE']}\")\n\n# --- 1. DATASET CLASS (RARE HUNTER) ---\nclass MADOSRareHunter(Dataset):\n    def __init__(self, file_pair_list, transform=None, crop_size=512, mode='train', bias=0.8):\n        self.file_list = file_pair_list\n        self.transform = transform\n        self.crop_size = crop_size\n        self.mode = mode \n        self.bias = bias \n        self.rare_classes = [1, 2, 3, 4, 5, 6, 9, 12, 13, 14, 15]\n\n    def __len__(self): return len(self.file_list)\n\n    def __getitem__(self, idx):\n        img_path, mask_path = self.file_list[idx]\n        \n        with rasterio.open(img_path) as src:\n            image = src.read().astype(np.float32)\n            image = np.nan_to_num(image)\n            img_min, img_max = image.min(), image.max()\n            if img_max > img_min:\n                image = (image - img_min) / ((img_max - img_min) + 1e-6)\n            else:\n                image = np.zeros_like(image)\n\n        with rasterio.open(mask_path) as src:\n            mask = src.read(1).astype(np.int64)\n            \n        if self.mode == 'train':\n            c, h, w = image.shape\n            unique_classes = np.unique(mask)\n            present_rare = np.intersect1d(unique_classes, self.rare_classes)\n            \n            if len(present_rare) > 0 and random.random() < self.bias:\n                target_cls = np.random.choice(present_rare)\n                indices = np.argwhere(mask == target_cls)\n                center = indices[random.randint(0, len(indices)-1)]\n                top = max(0, min(h - self.crop_size, center[0] - self.crop_size // 2))\n                left = max(0, min(w - self.crop_size, center[1] - self.crop_size // 2))\n            else:\n                top = random.randint(0, h - self.crop_size) if h > self.crop_size else 0\n                left = random.randint(0, w - self.crop_size) if w > self.crop_size else 0\n        else:\n            c, h, w = image.shape\n            top = max(0, (h - self.crop_size) // 2)\n            left = max(0, (w - self.crop_size) // 2)\n\n        image = image[:, top:top+self.crop_size, left:left+self.crop_size]\n        mask = mask[top:top+self.crop_size, left:left+self.crop_size]\n        \n        if image.shape[1] < self.crop_size or image.shape[2] < self.crop_size:\n            pad_h = max(0, self.crop_size - image.shape[1])\n            pad_w = max(0, self.crop_size - image.shape[2])\n            image = np.pad(image, ((0,0), (0,pad_h), (0,pad_w)))\n            mask = np.pad(mask, ((0,pad_h), (0,pad_w)))\n\n        if self.transform:\n            image = np.transpose(image, (1, 2, 0))\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            image = np.transpose(image, (2, 0, 1))\n            \n        return torch.tensor(image), torch.tensor(mask)\n\n# --- 2. SCENE-BASED SPLITTING (RIGOROUS) ---\ndef prepare_data():\n    print(\"Organizing Data (Scene Split)...\")\n    all_pairs = []\n    for img_path in glob('./data/MADOS_nearest/**/*_rhorc_*.tif', recursive=True):\n        if 'aux' in img_path: continue\n        mask_path = img_path.replace('_rhorc_', '_cl_')\n        if os.path.exists(mask_path): all_pairs.append((img_path, mask_path))\n\n    scene_ids = [os.path.basename(p[0]).split('_')[1] for p in all_pairs]\n    unique_scenes = sorted(list(set(scene_ids)))\n    train_scenes, val_scenes = train_test_split(unique_scenes, test_size=0.2, random_state=42)\n\n    train_list = [p for p in all_pairs if os.path.basename(p[0]).split('_')[1] in train_scenes]\n    val_list = [p for p in all_pairs if os.path.basename(p[0]).split('_')[1] in val_scenes]\n    \n    print(f\"   Train Scenes: {len(train_scenes)} | Val Scenes: {len(val_scenes)}\")\n    return train_list, val_list\n\ntrain_list, val_list = prepare_data()\n\n# --- 3. LOADERS ---\ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),\n    A.GridDistortion(p=0.3), A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3)\n])\nval_transform = A.Compose([A.CenterCrop(512, 512)])\n\ntrain_ds = MADOSRareHunter(train_list, transform=train_transform, mode='train', bias=0.8)\nval_ds = MADOSRareHunter(val_list, transform=val_transform, mode='val')\n\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n\n# --- 4. MODEL & LOSS ---\nprint(\"Building SegFormer...\")\nmodel = smp.Segformer(encoder_name=\"mit_b3\", encoder_weights=\"imagenet\", in_channels=11, classes=CONFIG['NUM_CLASSES']).to(CONFIG['DEVICE'])\n\n# EMA SETUP (Exponential Moving Average)\nema = ExponentialMovingAverage(model.parameters(), decay=0.999)\n\n# Loss Configuration\nweights = torch.ones(CONFIG['NUM_CLASSES']).float().to(CONFIG['DEVICE'])\nfor c in [1, 2, 3, 4, 9, 14]: weights[c] = 10.0\nfor c in [5, 12, 13, 15]: weights[c] = 5.0\nweights[6] = 2.0; weights[7] = 1.5; weights[0] = 0.05 # Background enabled\n\ncriterion_phase1 = nn.CrossEntropyLoss(weight=weights)\ncriterion_phase2 = smp.losses.LovaszLoss(mode=\"multiclass\", ignore_index=0)\n\noptimizer = AdamW(model.parameters(), lr=CONFIG['LR'])\nscheduler = CosineAnnealingLR(optimizer, T_max=CONFIG['EPOCHS'])\n\nprint(\"Scientific Setup Complete: Scene Split + EMA + True VSCP.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:03:50.490151Z","iopub.execute_input":"2025-12-01T06:03:50.490426Z","iopub.status.idle":"2025-12-01T06:04:04.3122Z","shell.execute_reply.started":"2025-12-01T06:03:50.490405Z","shell.execute_reply":"2025-12-01T06:04:04.311537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === TRAINING LOOP  ===\nimport sys\nimport random\nfrom tqdm import tqdm\nimport torch.nn.utils\nfrom torch.cuda.amp import autocast, GradScaler \n\ndef apply_vscp_batch(images, masks):\n    batch_size = images.shape[0]\n    if batch_size < 2: return images, masks\n    \n    half = batch_size // 2\n    imgs_A = images[:half].clone()\n    imgs_B = images[half:].clone()\n    masks_A = masks[:half].clone()\n    masks_B = masks[half:].clone()\n    \n    pixels_to_copy = (masks_B > 0)\n    mask_expanded = pixels_to_copy.unsqueeze(1).expand_as(imgs_A)\n    \n    imgs_A[mask_expanded] = imgs_B[mask_expanded]\n    masks_A[pixels_to_copy] = masks_B[pixels_to_copy]\n    \n    images[:half] = imgs_A\n    masks[:half] = masks_A\n    \n    return images, masks\n\nMAX_GRAD_NORM = 1.0\nVSCP_PROB = 0.5 \nscaler = torch.amp.GradScaler('cuda')\n\nbest_loss_p1 = float('inf')\nbest_loss_p2 = float('inf')\n\nprint(f\"Starting Training ({CONFIG['EPOCHS']} Epochs)...\")\n\nfor epoch in range(CONFIG['EPOCHS']):\n    model.train()\n    train_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['EPOCHS']}\")\n    \n    if epoch < CONFIG['PHASE_SWITCH_EPOCH']:\n        criterion = criterion_phase1\n        phase_name = \"Hunter (CE)\"\n    else:\n        criterion = criterion_phase2\n        phase_name = \"Sculptor (Lovasz)\"\n    loop.set_description(f\"Ep {epoch+1} [{phase_name}]\")\n    \n    for batch_idx, (images, masks) in enumerate(loop):\n        images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE']).long()\n        masks[masks >= CONFIG['NUM_CLASSES']] = 0\n        \n        if random.random() < CONFIG['VSCP_PROB']:\n            images, masks = apply_vscp_batch(images, masks)\n        \n        with autocast():\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n        \n        if torch.isnan(loss):\n            print(f\"\\nNaN detected!\"); sys.exit()\n            \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        ema.update()\n        \n        train_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    val_loss = 0\n    with torch.no_grad():\n        with ema.average_parameters():\n            for images, masks in val_loader:\n                images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE']).long()\n                masks[masks >= CONFIG['NUM_CLASSES']] = 0\n                with autocast():\n                    outputs = model(images)\n                    val_loss += criterion(outputs, masks).item()\n    \n    avg_val = val_loss / len(val_loader)\n    scheduler.step()\n    \n    print(f\"    {phase_name} Val Loss (EMA): {avg_val:.4f}\")\n    \n    if epoch < CONFIG['PHASE_SWITCH_EPOCH']:\n        if avg_val < best_loss_p1:\n            best_loss_p1 = avg_val\n            with ema.average_parameters():\n                torch.save(model.state_dict(), \"segformer_phase1_hunter.pth\")\n            print(\"    Phase 1 EMA Model Updated!\")\n    else:\n        if avg_val < best_loss_p2:\n            best_loss_p2 = avg_val\n            with ema.average_parameters():\n                torch.save(model.state_dict(), \"segformer_phase2_sculptor.pth\")\n            print(\"    Phase 2 EMA Model Updated!\")\n\nprint(\"Training Finished!\")\n\n# # === CELL 3b: RESUME TRAINING (BUG FIXED) ===\n# import torch\n# import sys\n# import random\n# from tqdm import tqdm\n\n# print(\"Resuming from Phase 2 Checkpoint...\")\n# if os.path.exists(\"segformer_phase2_sculptor.pth\"):\n#     model.load_state_dict(torch.load(\"segformer_phase2_sculptor.pth\"))\n#     print(\"Loaded 'segformer_phase2_sculptor.pth'. Resuming refinement...\")\n# else:\n#     print(\"Checkpoint not found. Starting Phase 2 from scratch (using current weights).\")\n\n# START_EPOCH = 23 \n# TOTAL_EPOCHS = 40\n# criterion = criterion_phase2\n# phase_name = \"Sculptor (Resume)\"\n\n# for epoch in range(START_EPOCH, TOTAL_EPOCHS):\n#     model.train()\n#     train_loss = 0\n#     loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\")\n    \n#     for batch_idx, (images, masks) in enumerate(loop):\n#         images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE']).long()\n#         masks[masks >= CONFIG['NUM_CLASSES']] = 0\n        \n#         if random.random() < CONFIG['VSCP_PROB']:\n#             images, masks = apply_vscp_batch(images, masks)\n        \n#         with autocast():\n#             outputs = model(images)\n#             loss = criterion(outputs, masks)\n        \n#         if torch.isnan(loss).any():\n#             print(f\"\\nNaN detected!\"); sys.exit()\n\n#         optimizer.zero_grad()\n#         scaler.scale(loss).backward()\n#         scaler.unscale_(optimizer)\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         scaler.step(optimizer)\n#         scaler.update()\n        \n#         ema.update()\n        \n#         train_loss += loss.item()\n#         loop.set_postfix(loss=loss.item())\n    \n#     val_loss = 0\n#     with torch.no_grad():\n#         with ema.average_parameters():\n#             for images, masks in val_loader:\n#                 images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE']).long()\n#                 masks[masks >= CONFIG['NUM_CLASSES']] = 0\n#                 with autocast():\n#                     outputs = model(images)\n#                     val_loss += criterion(outputs, masks).item()\n    \n#     avg_val = val_loss / len(val_loader)\n#     scheduler.step()\n#     print(f\"   ðŸ“‰ {phase_name} Val Loss (EMA): {avg_val:.4f}\")\n    \n#     if avg_val < best_loss_p2:\n#         best_loss_p2 = avg_val\n#         with ema.average_parameters():\n#             torch.save(model.state_dict(), \"segformer_phase2_sculptor.pth\")\n#         print(\"   Phase 2 EMA Model Updated!\")\n\n# print(\"Resume Finished!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:29:08.527358Z","iopub.execute_input":"2025-12-01T06:29:08.527679Z","iopub.status.idle":"2025-12-01T09:11:19.809402Z","shell.execute_reply.started":"2025-12-01T06:29:08.527651Z","shell.execute_reply":"2025-12-01T09:11:19.808395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === EVALUATION ===\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nCLASS_NAMES = {\n    0: \"Background\", 1: \"Marine Debris (Plastic)\", 2: \"Dense Sargassum\",\n    3: \"Sparse Floating Algae\", 4: \"Natural Organic Material\", 5: \"Ship\",\n    6: \"Oil Spill\", 7: \"Marine Water\", 8: \"Sediment-Laden Water\",\n    9: \"Foam\", 10: \"Turbid Water\", 11: \"Shallow Water\",\n    12: \"Waves & Wakes\", 13: \"Oil Platform\", 14: \"Jellyfish\", 15: \"Sea Snot\"\n}\n\nclass MADOSUnseenDataset(Dataset):\n    def __init__(self, file_list): self.file_list = file_list\n    def __len__(self): return len(self.file_list)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.file_list[idx]\n        with rasterio.open(img_path) as src:\n            image = src.read().astype(np.float32)\n            image = np.nan_to_num(image)\n            if image.max() > image.min():\n                image = (image - image.min()) / ((image.max() - image.min()) + 1e-6)\n        with rasterio.open(mask_path) as src:\n            mask = src.read(1).astype(np.int64)\n        \n        c, h, w = image.shape\n        top, left = max(0, (h - 512) // 2), max(0, (w - 512) // 2)\n        image = image[:, top:top+512, left:left+512]\n        mask = mask[top:top+512, left:left+512]\n        \n        if image.shape[1] < 512 or image.shape[2] < 512:\n            pad_h, pad_w = max(0, 512 - image.shape[1]), max(0, 512 - image.shape[2])\n            image = np.pad(image, ((0,0), (0,pad_h), (0,pad_w)))\n            mask = np.pad(mask, ((0,pad_h), (0,pad_w)))\n            \n        return torch.tensor(image), torch.tensor(mask)\n\ndef predict_with_tta(model, image):\n    model.eval()\n    logits_list = []\n    rotations = [0, 1, 2, 3]\n    with torch.no_grad():\n        for k in rotations:\n            img_rot = torch.rot90(image, k=k, dims=[2, 3])\n            out_rot = model(img_rot)\n            logits_list.append(torch.rot90(out_rot, k=-k, dims=[2, 3]))\n            \n            img_flip = torch.flip(img_rot, dims=[3])\n            out_flip = model(img_flip)\n            out_unflip = torch.flip(out_flip, dims=[3])\n            logits_list.append(torch.rot90(out_unflip, k=-k, dims=[2, 3]))\n            \n    return torch.mean(torch.stack(logits_list), dim=0)\n\ndef evaluate_rigorous(model, file_list, device, num_classes):\n    ds = MADOSUnseenDataset(file_list)\n    loader = DataLoader(ds, batch_size=4, shuffle=False, num_workers=2)\n    \n    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64, device=device)\n    print(f\"ðŸ“Š Evaluating...\")\n    \n    with torch.no_grad():\n        for images, masks in tqdm(loader):\n            images = images.to(device)\n            masks = masks.to(device).long()\n            \n            outputs = predict_with_tta(model, images)\n            preds = torch.argmax(outputs, dim=1)\n            \n            valid = (masks != 0)\n            if valid.sum() == 0: continue\n            \n            y_true, y_pred = masks[valid].flatten(), preds[valid].flatten()\n            bincount = torch.bincount(num_classes * y_true + y_pred, minlength=num_classes**2)\n            cm += bincount.reshape(num_classes, num_classes)\n            \n    return cm\n\nif 'val_list' not in locals():\n    print(\"Error: 'val_list' missing. Run Cell 2 first.\")\nelse:\n    conf_mat = evaluate_rigorous(model, val_list, CONFIG[\"DEVICE\"], CONFIG[\"NUM_CLASSES\"])\n    cm = conf_mat.cpu().numpy()\n    \n    TP = np.diag(cm)\n    FP = cm.sum(axis=0) - TP\n    FN = cm.sum(axis=1) - TP\n    eps = 1e-6\n    \n    IoU = TP / (TP + FP + FN + eps)\n    F1 = 2 * TP / (2 * TP + FP + FN + eps)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL RESULTS\")\n    print(\"=\"*60)\n    print(f\"mIoU (1-15): {np.nanmean(IoU[1:]):.4f}\")\n    print(f\"Mean F1 (1-15):  {np.nanmean(F1[1:]):.4f}\")\n    print(\"-\" * 60)\n    \n    stats = []\n    for i in range(1, CONFIG[\"NUM_CLASSES\"]):\n        stats.append({\n            \"Class\": CLASS_NAMES[i], \n            \"IoU\": IoU[i], \n            \"F1\": F1[i], \n            \"Pixels\": cm[i, :].sum()\n        })\n        \n    print(pd.DataFrame(stats).sort_values(by=\"IoU\", ascending=False).to_string(index=False, formatters={\"IoU\": \"{:.4f}\".format, \"F1\": \"{:.4f}\".format}))\n\n    plt.figure(figsize=(14, 10))\n    cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)\n    \n    sns.heatmap(cm_norm[1:, 1:], annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=[CLASS_NAMES[i] for i in range(1, 16)],\n                yticklabels=[CLASS_NAMES[i] for i in range(1, 16)])\n    \n    plt.title(\"Normalized Confusion Matrix\")\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.xticks(rotation=45, ha='right')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T09:29:52.546003Z","iopub.execute_input":"2025-12-01T09:29:52.546289Z","iopub.status.idle":"2025-12-01T09:34:04.968638Z","shell.execute_reply.started":"2025-12-01T09:29:52.546269Z","shell.execute_reply":"2025-12-01T09:34:04.967906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === VISUALIZATION ===\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.colors as mcolors\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport rasterio\nimport numpy as np\nfrom glob import glob\nimport os\n\nCONFIDENCE_THRESHOLD = 0.85\nDEBRIS_CLASSES = [1, 2, 3, 4, 5, 6, 9, 13, 14, 15]\n\nclass MADOSFullSceneDataset(Dataset):\n    def __init__(self, root_dir):\n        self.samples = []\n        for img_path in glob(os.path.join(root_dir, '**', '*_rhorc_*.tif'), recursive=True):\n            if 'aux' in img_path: continue\n            mask_path = img_path.replace('_rhorc_', '_cl_')\n            if os.path.exists(mask_path): self.samples.append((img_path, mask_path))\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx): return self.samples[idx]\n\ndef predict_with_tta(model, image):\n    model.eval()\n    logits_list = []\n    with torch.no_grad():\n        for k in [0, 1, 2, 3]:\n            rot = torch.rot90(image, k=k, dims=[2, 3])\n            out = model(rot)\n            logits_list.append(torch.rot90(out, k=-k, dims=[2, 3]))\n            flip = torch.flip(rot, dims=[3])\n            out_flip = model(flip)\n            logits_list.append(torch.rot90(torch.flip(out_flip, dims=[3]), k=-k, dims=[2, 3]))\n    return torch.mean(torch.stack(logits_list), dim=0)\n\ndef predict_sliding_window(model, image_tensor, max_tile_size=512, overlap=1/3):\n    model.eval()\n    _, c, h, w = image_tensor.shape\n    \n    tile_size = 256 if (h < max_tile_size or w < max_tile_size) else max_tile_size\n    \n    target_h, target_w = ((h+31)//32)*32, ((w+31)//32)*32\n    pad_h, pad_w = target_h - h, target_w - w\n    if pad_h > 0 or pad_w > 0:\n        image_tensor = F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)\n        \n    padded_h, padded_w = image_tensor.shape[2:]\n    stride = int(tile_size * (1 - overlap))\n    \n    heatmap = torch.zeros((1, CONFIG[\"NUM_CLASSES\"], padded_h, padded_w), device=CONFIG[\"DEVICE\"])\n    countmap = torch.zeros((1, 1, padded_h, padded_w), device=CONFIG[\"DEVICE\"])\n    \n    with torch.no_grad():\n        for y in range(0, padded_h, stride):\n            for x in range(0, padded_w, stride):\n                y1, x1 = min(y, padded_h - tile_size), min(x, padded_w - tile_size)\n                y2, x2 = y1 + tile_size, x1 + tile_size\n                \n                tile = image_tensor[:, :, y1:y2, x1:x2]\n                output = predict_with_tta(model, tile)\n                \n                heatmap[:, :, y1:y2, x1:x2] += output\n                countmap[:, :, y1:y2, x1:x2] += 1.0\n                \n    heatmap /= countmap\n    \n    probs = F.softmax(heatmap, dim=1)\n    max_probs, preds = torch.max(probs, dim=1)\n    preds[max_probs < CONFIDENCE_THRESHOLD] = 0\n    \n    return preds.squeeze().cpu().numpy()[:h, :w]\n\ndef visualize_full_scene(dataset, idx):\n    img_path, mask_path = dataset[idx]\n    filename = os.path.basename(img_path)\n    \n    with rasterio.open(img_path) as src:\n        image = src.read().astype(np.float32)\n        image = np.nan_to_num(image)\n        if image.max() > image.min(): \n            image = (image - image.min()) / ((image.max() - image.min()) + 1e-6)\n            \n    with rasterio.open(mask_path) as src: \n        mask = src.read(1).astype(np.int64)\n    \n    print(f\"\\nProcessing {filename}\")\n    \n    img_tensor = torch.tensor(image).unsqueeze(0).to(CONFIG[\"DEVICE\"])\n    pred_mask = predict_sliding_window(model, img_tensor)\n    \n    debris_mask = pred_mask.copy()\n    is_debris = np.isin(debris_mask, DEBRIS_CLASSES)\n    debris_mask[~is_debris] = 0\n    \n    fig, ax = plt.subplots(1, 4, figsize=(32, 8))\n    \n    rgb = image[0:3].transpose(1, 2, 0)\n    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n    \n    cmap = plt.get_cmap(\"tab20\")\n    cmap_list = [cmap(i) for i in range(cmap.N)]; cmap_list[0] = (0, 0, 0, 1)\n    custom_cmap = mcolors.ListedColormap(cmap_list)\n    \n    ax[0].imshow(rgb); ax[0].set_title(\"Input RGB\"); ax[0].axis('off')\n    ax[1].imshow(mask, cmap=custom_cmap, vmin=0, vmax=20, interpolation='nearest')\n    ax[1].set_title(\"Ground Truth\"); ax[1].axis('off')\n    ax[2].imshow(pred_mask, cmap=custom_cmap, vmin=0, vmax=20, interpolation='nearest')\n    ax[2].set_title(f\"Prediction (Confidence > {CONFIDENCE_THRESHOLD})\"); ax[2].axis('off')\n    ax[3].imshow(debris_mask, cmap=custom_cmap, vmin=0, vmax=20, interpolation='nearest')\n    ax[3].set_title(\"Targeted Debris Extraction\"); ax[3].axis('off')\n    \n    unique = np.unique(np.concatenate((mask, pred_mask)))\n    patches = []\n    for c in unique:\n        if c == 0: continue\n        color = cmap_list[c] if c < 20 else (1,1,1,1)\n        patches.append(mpatches.Patch(color=color, label=f\"{c}: {CLASS_NAMES.get(c,'?')}\"))\n    \n    if patches: \n        ax[3].legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Running Visualization...\")\nfull_scene_ds = MADOSFullSceneDataset('./data/MADOS_nearest')\nfor i in range(3):\n    idx = np.random.randint(0, len(full_scene_ds))\n    visualize_full_scene(full_scene_ds, idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T09:34:25.110395Z","iopub.execute_input":"2025-12-01T09:34:25.11097Z","iopub.status.idle":"2025-12-01T09:34:29.80602Z","shell.execute_reply.started":"2025-12-01T09:34:25.110942Z","shell.execute_reply":"2025-12-01T09:34:29.805232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CELL 6: MINIMAL VISUALIZATION (FIXED VARIABLE NAMES) ===\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.colors as mcolors\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport rasterio\nimport numpy as np\nfrom glob import glob\nimport os\nfrom skimage import exposure\n\n# --- CONFIGURATION ---\nCONFIDENCE_THRESHOLD = 0.85\nDEBRIS_CLASSES = [1, 2, 3, 4, 5, 6, 9, 13, 14, 15]\n\nCLASS_NAMES = {\n    0: \"Background\", 1: \"Marine Debris (Plastic)\", 2: \"Dense Sargassum\",\n    3: \"Sparse Floating Algae\", 4: \"Natural Organic Material\", 5: \"Ship\",\n    6: \"Oil Spill\", 7: \"Marine Water\", 8: \"Sediment-Laden Water\",\n    9: \"Foam\", 10: \"Turbid Water\", 11: \"Shallow Water\",\n    12: \"Waves & Wakes\", 13: \"Oil Platform\", 14: \"Jellyfish\", 15: \"Sea Snot\"\n}\n\n# --- DATASET & INFERENCE UTILS ---\nclass MADOSFullSceneDataset(Dataset):\n    def __init__(self, root_dir):\n        self.samples = []\n        for img_path in glob(os.path.join(root_dir, '**', '*_rhorc_*.tif'), recursive=True):\n            if 'aux' in img_path: continue\n            mask_path = img_path.replace('_rhorc_', '_cl_')\n            if os.path.exists(mask_path): self.samples.append((img_path, mask_path))\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx): return self.samples[idx]\n\ndef predict_with_tta(model, image):\n    model.eval()\n    logits_list = []\n    with torch.no_grad():\n        for k in [0, 1, 2, 3]:\n            rot = torch.rot90(image, k=k, dims=[2, 3])\n            out = model(rot)\n            logits_list.append(torch.rot90(out, k=-k, dims=[2, 3]))\n            flip = torch.flip(rot, dims=[3])\n            out_flip = model(flip)\n            logits_list.append(torch.rot90(torch.flip(out_flip, dims=[3]), k=-k, dims=[2, 3]))\n    return torch.mean(torch.stack(logits_list), dim=0)\n\ndef predict_sliding_window(model, image_tensor, max_tile_size=512, overlap=1/3):\n    model.eval()\n    _, c, h, w = image_tensor.shape\n    tile_size = 256 if (h < max_tile_size or w < max_tile_size) else max_tile_size\n    target_h, target_w = ((h+31)//32)*32, ((w+31)//32)*32\n    pad_h, pad_w = target_h - h, target_w - w\n    if pad_h > 0 or pad_w > 0:\n        image_tensor = F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)\n    padded_h, padded_w = image_tensor.shape[2:]\n    stride = int(tile_size * (1 - overlap))\n    \n    # --- FIX: Use CONFIG Dictionary ---\n    heatmap = torch.zeros((1, CONFIG['NUM_CLASSES'], padded_h, padded_w), device=CONFIG['DEVICE'])\n    countmap = torch.zeros((1, 1, padded_h, padded_w), device=CONFIG['DEVICE'])\n    \n    with torch.no_grad():\n        for y in range(0, padded_h, stride):\n            for x in range(0, padded_w, stride):\n                y1, x1 = min(y, padded_h - tile_size), min(x, padded_w - tile_size)\n                y2, x2 = y1 + tile_size, x1 + tile_size\n                tile = image_tensor[:, :, y1:y2, x1:x2]\n                output = predict_with_tta(model, tile)\n                heatmap[:, :, y1:y2, x1:x2] += output\n                countmap[:, :, y1:y2, x1:x2] += 1.0\n    heatmap /= countmap\n    probs = F.softmax(heatmap, dim=1)\n    max_probs, preds = torch.max(probs, dim=1)\n    preds[max_probs < CONFIDENCE_THRESHOLD] = 0\n    return preds.squeeze().cpu().numpy()[:h, :w]\n\n# --- VISUALIZATION ---\ndef visualize_minimal_scene(dataset, idx):\n    img_path, mask_path = dataset[idx]\n    filename = os.path.basename(img_path)\n    \n    with rasterio.open(img_path) as src:\n        raw_image = src.read().astype(np.float32)\n    \n    print(f\"\\nðŸ“¸ Processing {filename}...\")\n    \n    # Model Input Norm\n    model_input = raw_image.copy()\n    model_input = np.nan_to_num(model_input)\n    if model_input.max() > model_input.min():\n        model_input = (model_input - model_input.min()) / ((model_input.max() - model_input.min()) + 1e-6)\n            \n    # Inference\n    img_tensor = torch.tensor(model_input).unsqueeze(0).to(CONFIG['DEVICE']) # FIX: Use CONFIG\n    pred_mask = predict_sliding_window(model, img_tensor)\n    \n    # Debris Mask\n    debris_mask = pred_mask.copy()\n    is_debris = np.isin(debris_mask, DEBRIS_CLASSES)\n    debris_mask[~is_debris] = 0\n    \n    if np.sum(is_debris) == 0:\n        print(\"   (Skipping visualization - No debris detected)\")\n        return\n\n    # RGB Processing\n    rgb = raw_image[[2, 1, 0], :, :].transpose(1, 2, 0)\n    p2, p98 = np.percentile(rgb, (2, 98))\n    rgb = np.clip((rgb - p2) / (p98 - p2), 0, 1)\n    rgb = exposure.adjust_gamma(rgb, 0.8)\n\n    # Plotting\n    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n    \n    cmap = plt.get_cmap(\"tab20\")\n    cmap_list = [cmap(i) for i in range(cmap.N)]; cmap_list[0] = (0, 0, 0, 1)\n    custom_cmap = mcolors.ListedColormap(cmap_list)\n    \n    ax[0].imshow(rgb); ax[0].set_title(\"Satellite Input (Enhanced RGB)\"); ax[0].axis('off')\n    ax[1].imshow(debris_mask, cmap=custom_cmap, vmin=0, vmax=20, interpolation='nearest')\n    ax[1].set_title(f\"Detected Debris (Conf > {CONFIDENCE_THRESHOLD})\"); ax[1].axis('off')\n    \n    unique = np.unique(debris_mask)\n    patches = [mpatches.Patch(color=cmap_list[c], label=f\"{c}: {CLASS_NAMES.get(c,'?')}\") for c in unique if c!=0]\n    if patches: ax[1].legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    plt.tight_layout(); plt.show()\n\n# Execute\nprint(\"ðŸ‘€ Searching for scenes with debris...\")\nfull_scene_ds = MADOSFullSceneDataset('./data/MADOS_nearest')\ncount = 0\nfor _ in range(100): \n    idx = np.random.randint(0, len(full_scene_ds))\n    visualize_minimal_scene(full_scene_ds, idx)\n    # Check if a plot was actually created (matplotlib keeps track of open figures)\n    if plt.get_fignums(): \n        count += 1\n        if count >= 3: break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T09:44:10.467743Z","iopub.execute_input":"2025-12-01T09:44:10.468516Z","iopub.status.idle":"2025-12-01T09:46:18.161991Z","shell.execute_reply.started":"2025-12-01T09:44:10.46849Z","shell.execute_reply":"2025-12-01T09:46:18.161127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}